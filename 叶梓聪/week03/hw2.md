# HW2_叶梓聪

## 1. 正则表达式

**优点:**

*   **简单快速**：实现起来非常直接，不需要训练数据，执行速度极快，计算资源消耗很低。
*   **准确性高**：对于模式非常固定的文本，正则表达式可以做到J较高的准确匹配。
*   **可解释性强**：人工编写规则，每一条规则的匹配逻辑都清晰明了，非常容易理解和调试。

**缺点:**

*   **泛化能力差**：它只能识别预先定义好的模式，对于用户稍微变换一下说法，或者有错别字，正则表达式就不行了。
*   **维护成本高**：随着意图类别和语料的增加，需要编写和维护的规则数量会急剧膨胀，规则之间可能会产生冲突，导致管理变得非常困难。
*   **无法处理复杂语义**：正则表达式是基于字符串匹配的，无法理解文本的深层语义。



## 2. TF-IDF

**优点:**

*   **效果不错**：，TF-IDF结合简单的分类器就能达到相当不错的分类效果。
*   **训练速度快**：相比深度学习模型，它的训练速度要快得多，对计算资源的要求也较低。
*   **可解释性尚可**：可以通过分析TF-IDF值和分类器权重，来理解哪些词对分类结果的贡献最大。

**缺点:**

*   **忽略语序和语义**：TF-IDF只关心词语的出现频率，而完全忽略了词语在句子中的顺序和上下文关系，因此丢失了大量的句法和语义信息。
*   **数据稀疏性问题**：对于大规模的词汇表，生成的TF-IDF向量会非常长且高度稀疏。
*   **依赖分词**：对于中文等语言，TF-IDF的效果高度依赖于分词的准确性。



## 3. BERT

**优点:**

*   **强大的语义理解能力**：BERT能够捕捉词语在不同上下文中的细微差别，并理解复杂的句法结构和语义关系。
*   **性能优越**：经过微调的BERT模型能够超越传统的机器学习方法。
*   **端到端**：只需要关心输入和输出，不需要进行复杂的人工特征工程。

**缺点:**

*   **计算成本高**：BERT模型参数量巨大，都需要消耗大量的计算资源，速度相对较慢。
*   **需要大量数据**：要达到好的效果，需要一定数量的高质量标注数据。
*   **可解释性差**：难以精确地解释为什么模型会做出某个特定的预测。



## 4. PROMPT

**优点:**

*   **少样本能力**：只需要设计好Prompt，少样本情况下，模型也能直接给出正确答案，或者只需要在Prompt中提供几个例子，就能达到非常好的效果。
*   **灵活性和通用性极高**：同一个大语言模型可以用来解决各种各样的NLP任务，如分类，翻译，写作，只需要改变Prompt即可，无需重新训练模型。

**缺点:**

*   **依赖外部API**：服务依赖、数据隐私和成本问题。
*   **结果不可控**：可能会生成一些意料之外的、格式不规范甚至错误的内容。
            
